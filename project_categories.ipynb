{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"50px\" align=\"left\" style=\"margin-right:20px\" src=\"http://data.newprolab.com/public-newprolab-com/npl_logo.png\"> <b>New Professions Lab</b> <br /> Специалист по большим данным"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Проект 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Спрогнозировать пол и возрастную категорию интернет-пользователей по логу посещения сайтов"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img width=\"110px\" align=\"left\" src=\"http://data.newprolab.com/public-newprolab-com/project01_img0.png?img\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/newprolab/content_bigdata7\"><img align=\"left\" src=\"http://data.newprolab.com/public-newprolab-com/npl7.svg\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из задач DMP-системы состоит в том, чтобы по разрозненным даннным, таким, как посещения неким пользователем сайтов, классифицировать его и присвоить ему определённую категорию: пол, возраст, интересы и так далее. В дальнейшем составляется портрет, или профиль, пользователя, на основе которого ему более таргетированно показывается реклама в интернете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Задача"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Используя доступный набор данных о посещении страниц у одной части пользователей, сделать прогноз относительно **пола и возрастной категории** другой части пользователей. Угадывание (hit) - правильное предсказание и пола, и возрастной категории одновременно.\n",
    "\n",
    "Мы не ограничиваем вас в выборе инструментов и методов работы с данными. Используйте любые эвристики, внешние источники, парсинг контента страниц — всё, что поможет вам выполнить задачу. Единственное ограничение — никаких ручных действий. Руками проставлять классы нельзя.\n",
    "\n",
    "Поскольку это ваш проект, который мы наверняка захотите показать другим, уделите его оформлению достаточно времени. Мы рекомендуем сделать весь проект в этом ноутбуке. Снизу, под заданием, вы сможете описать ваше решение.\n",
    "\n",
    "⏰ **Дедлайн: 06 ноября 2017, 23:59**"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import re\n",
    "import os\n",
    "import json\n",
    "from urlparse import urlparse\n",
    "from urllib import urlretrieve, unquote"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных на вход"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Для выполнения работы вам следует взять файл http://data.newprolab.com/data-newprolab-com/project01/gender_age_dataset.txt и положить к себе в директорию `data`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# First, we download the input data for the project:\n",
    "\n",
    "data_dir = 'data'\n",
    "filename = 'gender_age_dataset.txt'\n",
    "file_path = '/'.join([data_dir,filename])\n",
    "url = 'http://data.newprolab.com/data-newprolab-com/project01/' + filename\n",
    "\n",
    "if not os.path.isdir(data_dir): os.mkdir(data_dir)\n",
    "if not os.path.isfile(file_path):\n",
    "    print('Downloading ' + filename + '...')\n",
    "    urlretrieve(url, file_path)\n",
    "    print('Download completed')\n",
    "\n",
    "# Wait until you see that all files have been downloaded.\n",
    "print('OK')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Он содержит данные о посещении сайтов ~40 000 пользователей, при этом по некоторым из них (~ 35 000) известны их пол и возрастная категория, а по 5 000 - эта информация не известна. В файле есть 4 поля:\n",
    "* **gender** - пол, принимающий значения `M` (male - мужчина), `F` (female - женщина), `-` (пол неизвестен);\n",
    "* **age** - возраст, представленный в виде диапазона x-y (строковый тип), или `-` (возрастная категория неизвестна);\n",
    "* **uid** - идентификатор пользователя, строковая переменная;\n",
    "* **user_json** - поле json, в котором содержатся записи о посещении сайтов этим пользователем `(url, timestamp)`.\n",
    "\n",
    "Первое, что обычно делают в таких случаях, — исследуют имеющийся датасет и разбираются, какие же данные мы получили."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Загрузим весь датасет в pandas:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df = pd.read_csv(file_path, sep='\\t')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "И теперь попробуем понять, что у нас есть:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Что содержится в `user_json`?"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "df.iloc[0].user_json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Видим, что это некая сериализованная json-строка, которую можно легко разобрать через модуль `json`:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "json.loads(df.iloc[0].user_json)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Методом `pandas.DataFrame.apply` (хотя не только им) можно применить операцию десериализации json-строк ко всему датасету. Рекомендуем почитать [документацию по методу apply](http://pandas.pydata.org/pandas-docs/stable/generated/pandas.DataFrame.apply.html).\n",
    "\n",
    "Работая с подобными операциями, обратите внимание на kwargs-аргумент `axis`. Часто, забыв его указать, вы примените операцию не к ряду (строке), а к столбцу, что вряд ли входит в ваши планы."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Очистка данных и feature engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Очистка данных и генерация новых фич составит значительную часть вашей работы. Именно здесь вы и должны продемонстрировать знания и креативность: чем лучше окажутся ваши фичи и чем лучше сможете убрать шум из датасета, тем лучших результатов вы достигнете."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Одна из первых вещей, которые можно попробовать — это вытащить домены и использовать их в качестве признаков. Можно воспользоваться функцией:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "def url2domain(url):\n",
    "    url = re.sub('(http(s)*://)+', 'http://', url)\n",
    "    parsed_url = urlparse(unquote(url.strip()))\n",
    "    if parsed_url.scheme not in ['http','https']: return None\n",
    "    netloc = re.search(\"(?:www\\.)?(.*)\", parsed_url.netloc).group(1)\n",
    "    if netloc is not None: return str(netloc.encode('utf8')).strip()\n",
    "    return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Поскольку эта часть и есть ваша работа, мы не станем раскрывать все секреты (хотя несколько советов мы всё же дали, посмотрите ниже в разделе Подсказки)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Деление на train и test сеты, обучение модели, предсказания для test-сета"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Давайте теперь оценим размер нашего train и test сетов. Train set:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(df[~((df.gender == '-') & (df.age == '-'))])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Test set:"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "len(df[(df.gender == '-') & (df.age == '-')])"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "len(df) # Весь датасет"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Когда вы очистили данные и сгенерировали признаки, которые можно дать на вход алгоритму, следующий этап — это разделить данные на тренировочную и тестовую выборки. Сохраните train и test выборки в отдельных файлах, используя метод `pandas.DataFrame.to_csv`. Либо просто сделайте два датафрейма: `train_df` и `test_df`. Обучите модель на ваш выбор, оцените результат, подумайте, как можно его улучшить."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Обработка данных на выход\n",
    "Выходной файл должен быть расположен в корне вашей директории в файле `project01_gender-age.csv`. Чекер будет брать файл именно оттуда.\n",
    "\n",
    "Файл должен содержать три поля: `uid` (строковый формат), `gender` (строковый формат) и `age` (строковый формат). \n",
    "\n",
    "В файле должны быть только те пользователи, у которых пол и возрастная категория изначально неизвестны, и они должны быть **отсортированы по UID по возрастанию значений лексикографически.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Пример выходного файла:\n",
    "\n",
    "```uid\tgender\tage\n",
    "123\tF\t18-24\n",
    "456\tM\t25-34\n",
    "789\t-\t-\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файл обязательно должен содержать шапку, указанную выше, и все 5 000 записей по неизвестным пользователям. Итого: 5 001 строка."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Подсказки\n",
    "\n",
    "1. Есть много различных способов решить данную задачу: можно просто хорошо поработать с урлами и доменами, можно пропарсить содержимое этих урлов (заголовки, текст и т.д.) и воспользоваться неким векторизатором типа TF\\*IDF для генерации дополнительных фич, которые уже в дальнейшем вы подадите на вход ML-алгоритму, можно сделать тематическое моделирование (LDA, BigARTM) сайтов и использовать одну или несколько тем в качестве фич.\n",
    "\n",
    "2. Возможно, что данные грязные и их нужно дополнительно обработать. Спецсимволы, кириллические домены? Уделите этому этапу достаточно времени: здесь чистота датасета важнее, чем выбор алгоритма.\n",
    "\n",
    "3. Часто бывает, что лучшее решение с точки зрения результата — оно же самое простое. Попробуйте сначала простые способы, простые алгоритмы, прежде чем переходить к тяжёлой артиллерии. Один из вариантов — начать с небольшого RandomForest.\n",
    "\n",
    "4. Вам почти наверняка понадобится что-то из пакета sklearn. [Документация](http://scikit-learn.org/stable/user_guide.html) — ваш лучший друг.\n",
    "\n",
    "5. Вы можете сначала предсказать пол, а затем возраст, либо сразу и то, и другое. Экспериментируйте.\n",
    "\n",
    "6. В Python 2.7 возможны проблемы с юникодом. Способы решения существуют — обращайтесь в slack за советами.\n",
    "\n",
    "7. Объединяйтесь в команды. Так гораздо веселее и интереснее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Проверка\n",
    "Проверка осуществляется из [Личного кабинета](http://lk.newprolab.com/lab/project01). UID должны идти в первоначальном порядке. По файлу будет определяться доля правильно спрогнозированных пользователей (у которых правильно указаны и пол, и возрастная категория).\n",
    "\n",
    "* В поле `part of users with predicted gender + age` - указана доля пользователей, которая была предсказана от общего числа неизвестных пользователей (пример: по 3 000 был сделан прогноз, а всего было неизвестно 5 000, чекер выдаст 0.6).\n",
    "\n",
    "* В поле `correctly predicted users / total number of users` - указана доля пользователей, которая была правильно предсказана (совпадает и пол, и возраст) от общего числа всех пользователей (пример: по 3 000 был сделан прогноз, правильно было спрогнозировано 1 500, а всего было неизвестно 5 000, чекер выдаст 0.3)\n",
    "\n",
    "* В поле `correctly predicted users / number of predicted users` - указана доля пользователей, которая была правильно предсказана (совпадает и пол, и возраст) от общего числа предсказанных пользователей (пример: по 3 000 был сделан прогноз, из них правильно предсказано 1 500, чекер выдаст 0.5).\n",
    "\n",
    "**Важное замечание!** Вы должны дать прогноз хотя бы по 50% пользователей, у которых изначально не указан пол и возрастная категория. Иными словами, вы можете оставить неопределенными не более 50% изначально неопределенных пользователей.\n",
    "\n",
    "**Если доля в последнем поле превысит порог 0.28, то проект будет засчитан.**\n",
    "\n",
    "Лучшей команде, набравшей максимальный результат, мы подарим специальный приз, о котором скажем позднее."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "## Ваше решение здесь"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Для начала, возьмем базовый набор, разделим его на наборы для известных пола и возраста и неизвестных.\n",
    "По дороге, вытащим из json сайт, путь и запрос. Одного сайта недостаточно, вероятность правильного определения только по сайту не так хороша, как хотелось.\n",
    "Первой итерацией, я выделял только сайт, определял для каждого вероятность М/Ж и возраста, отсекал те, у кого максимум вероятности был невелик или число использований этого сайта было мало. А потом, для неизвестных считался результат - для каждого сайта брались вероятности, суммировались с вероятностями других сайтов и наибольшая сумма определяла победителя.\n",
    "Примерно так:\n",
    "  iLRes = [0,0] # - для uid пара м/ж\n",
    "  for line in fSrcFile: # - для каждой строки файла с неизвестными (отсортированного по uid) соверши reduce подсчет\n",
    "    line = line.strip()\n",
    "    sUid, sHost = line.split(\",\")\n",
    "    if (sUid != sLastUid and sLastUid is not None):\n",
    "      if iLRes[0] > iLRes[1]:\n",
    "        sGender = 'M'\n",
    "      else:\n",
    "        sGender = 'F'\n",
    "      if iLRes[0] + iLRes[1] > 0:\n",
    "        fPctM = round(float(iLRes[0] / (iLRes[0] + iLRes[1])),2)\n",
    "      else:\n",
    "        fPctM = -1.0\n",
    "        sGender = '-'\n",
    "    if sHost in dDict:\n",
    "      iLRes[0] += dDict[sHost][0]\n",
    "      iLRes[1] += dDict[sHost][1]\n",
    "\n",
    "Результат такого разбора: 0.1726 / 0.5016 / 0.344098883573\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "В итоговом решении, вместо всего этого, из json вырезаетcя host, path и query, отрезается лишнее и пилится на слова.\n",
    "Мысль в том, чтобы вот из этого \n",
    " http://zebra-zoya.ru/200028-chehol-organayzer-dlja-macbook-11-grid-it.html?\n",
    "     utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\"\n",
    "Сначала вырезать \n",
    "Host: zebra-zoya.ru\n",
    "Path: 200028-chehol-organayzer-dlja-macbook-11-grid-it.html\n",
    "Query: utm_campaign=397720794&utm_content=397729344&utm_medium=cpc&utm_source=begun\n",
    "\n",
    "Убрать ненужное (страновой домен, разделители из path, имена переменных из query) и собрать заново\n",
    "zebrazoya.chehol.organayzer.dlja.macbook.grid.html.begun\n",
    "\n",
    "Причем слова нужны как английские, так и русские\n",
    "Результат примерно такой:\n",
    "-- Ищем игрушки и отели\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,tonkosti.отели.геленджика.включено\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,tonkosti.отели.геленджика.включено\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,tonkosti.отели.геленджика.включено\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,tonkosti.отели.геленджика.включено\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,yandex.clck.jsredir\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,avito.moskva.tovary.dlya.detey.igrushki.kukla.eppl.vayt.shlyapnaya.vecherinka\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,labirint.books\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,avito.moskva.tovary.dlya.detey.igrushki.lizzi.harts.lizzie.hearts\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,avito.moskva.tovary.dlya.detey.igrushki\n",
    "bd7a6f52-45db-49bf-90f2-a3b07a9b7bcd,avito.moskva.tovary.dlya.detey.igrushki\n",
    "\n",
    "-- ищем инфинити, но меняем решение на patriot или крузак\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,cars.find.marka.infiniti\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,google.search.android.android.launcher.widget.инфинити.дизель.mobile.serp.msedr.mobile.serp.ovjuuneoa.wlvjhnkytaapcdgyad.инфинити.дизель\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,cars.find.marka.infiniti\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,google.search.android.android.launcher.widget.devloc.инфинити\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,uazservice.patriot.patriot.html\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,uazservice.dilers.tdmotors.html\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,uazservice.dilers.tdmotors.html\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,clck.yandex.jsredir.yandex.патриот.дизель.щербинке\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,narule.cars.hunter\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,narule.cars.hunter\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,clck.yandex.jsredir.yandex.хантер.дизель.москве.самой.полной.комплектации\n",
    "bd7c5d7a-0def-41d1-895f-fdb96c56c2d4,avito.moskva.avtomobili.probegom.toyota.land.cruiser\n",
    "\n",
    "Как видно, без query и path этого мы бы не увидели: были б только tonkosti.ru, avito.ru, cars.ru, google.ru"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Библиотеки и переменные\n",
    "import json\n",
    "from urllib.parse import urlparse\n",
    "from urllib.parse import parse_qs\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "from tqdm import tqdm\n",
    "import os\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import sys\n",
    "sPath = '/data/home/dmitry.lyubinskiy/data2/'\n",
    "sSrc = sPath+'sample.txt'\n",
    "sFirstKnown = sPath+'known_unsF.csv'\n",
    "sFirstUnKnown = sPath+'unk_unsF.csv'\n",
    "sSecondUnKnown =sPath+'unk_groupedF.csv'\n",
    "sSecondKnown =sPath+'known_groupedF.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Функции для зачистки данных. Местами написано криво, но работает\n",
    "# где-то навырезал лишнего, например убираю слова меньше 3 знаков, ушли htc, car и т.д.\n",
    "# Чтобы zebra-zoya не превращалась в одно, стоило добавить в reg \"-\", а потом обработать результат Splitter\n",
    "# Стоило ли убирать все цифры, в т.ч. сайты с цифрами. Я считаю, что стоило. Как и с тире и с 3 буквами\n",
    "import re\n",
    "reg = re.compile('[^a-zA-Zа-яА-Я .]')\n",
    "def Splitter(pText):\n",
    "  delimiters = ['\\n','-', ' ', ',', '.', '?', '!','\\\\','/', ':','=','_','%2f']\n",
    "  words = re.split('|\\\\'.join(delimiters), pText)\n",
    "  if len(pText)/len(words) > 20:\n",
    "    return ''\n",
    "  return '.'.join(words)\n",
    "def ClearUrl(pUrl):\n",
    "    # Вот этот реплейс писался несколько раз, причем сначала на питоне, потом на hive, потом в jupiter\n",
    "    # Вырезались откровенно косячные фрагменты адресов и сайты\n",
    "    sUrl = pUrl.lower().replace('http://https://','http://')\\\n",
    "           .replace('http://http://','http://').replace('http\\:\\/\\/\\&referrer=','')\\\n",
    "           .replace(\"www.\",\"\")\n",
    "    if not (sUrl.startswith('javascript:') \\\n",
    "               or sUrl.startswith('about:blank') \\\n",
    "               or sUrl.startswith('http://-') \\\n",
    "               or sUrl.startswith('http:// ') \\\n",
    "               or sUrl.startswith('${referer_url}') \\\n",
    "               or sUrl.startswith('http://(empty)')):\n",
    "      return sUrl\n",
    "    return ''\n",
    "\n",
    "def ParseUrl(pUrl):\n",
    "    sUrl = ClearUrl(pUrl)\n",
    "    if sUrl == '':\n",
    "       return sUrl\n",
    "    urlData = urlparse(sUrl) # Разделяем URL на запчасти\n",
    "    sHost=urlData.netloc # сам сайт\n",
    "    if sHost.count('.') > 0:\n",
    "      sHost = '.'.join(sHost.split('.')[:-1]) # адрес сайта берем без странового суффикса\n",
    "    sPath=urlData.path # путь \n",
    "    if sPath.count('/') > 0:\n",
    "      sPath = Splitter(sPath) # Path берем целиком, но убираем разделители\n",
    "    \n",
    "    sQuery=urlData.query\n",
    "    if sQuery.count('=') > 0:\n",
    "       sQuery = '.'.join([Splitter(x[0]) for x in parse_qs(sQuery).values()]) # из query берем только значени и выкидываем имена переменных\n",
    "    else:\n",
    "       sQuery = Splitter(sQuery) # А вот есть в query нет = то берем целиком\n",
    "    aSrc = reg.sub('','.'.join([sHost, sPath, sQuery])) # оставляем только буквы, пробелы и точки\n",
    "    aR = []\n",
    "    for sItem in aSrc.split('.'):\n",
    "      if len(sItem) > 3:\n",
    "        aR.append(sItem)\n",
    "    aRes ='.'.join(aR)\n",
    "\n",
    "    return aRes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "41138it [05:03, 135.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 4min 59s, sys: 4.31 s, total: 5min 3s\n",
      "Wall time: 5min 3s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "### Преобразовываем\n",
    "## Кодировка utf-8 тут для русских сайтов, а не для развлечения. Без нее, русские адреса резались и не получалось\n",
    "## полного комплекта uid и данных\n",
    "# открываем файлы\n",
    "fSrc = open(sSrc, 'r')\n",
    "fSrcGood = open(sFirstKnown, 'w', encoding='utf-8')\n",
    "fSrcUnk = open(sFirstUnKnown, 'w', encoding='utf-8')\n",
    "next(fSrc) # пропускаем заголовок\n",
    "for line in tqdm(fSrc):\n",
    "    # Каждую строку пилим на пол, возраст, uid и json\n",
    "    line = line.strip()\n",
    "    \n",
    "    gsGender, gsAge, gsUid, gsUsJson = line.split(\"\\t\")\n",
    "    \n",
    "    jsUserData = json.loads(gsUsJson)\n",
    "    for j2 in jsUserData['visits']:\n",
    "       # Из json берем каждый URL и собираем из него слова\n",
    "       urlUsData = ParseUrl(j2['url'])\n",
    "       if not (urlUsData == '' ) and (gsGender == 'M' or gsGender == 'F'):\n",
    "         ## В известные данные мы пишем только не пустые строки\n",
    "         sText = \",\".join([gsGender] + [gsAge] + [gsUid] + [urlUsData])\n",
    "         fSrcGood.write(str(sText)+\"\\n\")\n",
    "       elif gsGender == '-':\n",
    "         ## А вот в неизвестные - все, т.к. будут uid без нормальных данных\n",
    "         sText = \",\".join([gsUid] + [urlUsData])\n",
    "         fSrcUnk.write(str(sText)+\"\\n\") \n",
    "fSrcGood.close()\n",
    "fSrcUnk.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Для неизвестных. Группируем для uid список хостов через запятую, сохраняем\n",
    "pGood = pd.read_csv(sFirstUnKnown,header=0,names=['Uid','Host'],encoding='utf-8')\n",
    "pGood.sort_values(by='Uid',inplace=True)\n",
    "a=pGood[['Uid','Host']].groupby(['Uid'])['Host'].apply(list)\n",
    "a.to_csv(sSecondUnKnown, header=True, index=True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Для известных. Группируем для uid, age, gender список хостов через запятую, сохраняем\n",
    "pGood = pd.read_csv(sFirstKnown,header=0,names=['Gender','Age','Uid','Host'],encoding='utf-8')\n",
    "pGood.sort_values(by='Uid',inplace=True)\n",
    "a=pGood[['Uid','Gender','Age','Host']].groupby(['Uid','Gender','Age'])['Host'].apply(list)\n",
    "a.to_csv(sSecondKnown, header=True, index=True,encoding='utf-8')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Теперь у нас есть два файла - известный (gender, age, uid, host) и неизвестный (uid, host). В host находится массив строк для каждого сайта. Файлы УЖЕ отсортированы, дальше к сортировке не возвращаемся. На выходе отсортируем для верности, но и всё"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Дальше я разделил работу на две части - построение ML для возраста и для пола по отдельности. Векторайзеры также у каждого свои.\n",
    "Сама работа была итеративна и результаты сохранялись в пиклах и поднимались из них.\n",
    "\n",
    "Стоп слова для векторайзеров единые.\n",
    "Эстиматоры искались через гриды. Победил XGBClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# читаем файл\n",
    "pKnown = pd.read_csv(sSecondKnown,encoding='utf-8')\n",
    "# пишем стоп слова\n",
    "stop_w=['html','search','index','avito','mail','rambler','forum'\\\n",
    "       ,'yandex','vk','news','msk','ru','com','google','xn','spb'\\\n",
    "       ,'facebook','russian','page','catalog','list','open','view'\\\n",
    "       ,'video','http','sankt','peterburg','jsredir','user','thread'\\\n",
    "       ,'images','inbox','banner','results','aspx','main','online','phtml'\\\n",
    "       ,'login','post','email','item','iframe']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 28.2 s, sys: 509 ms, total: 28.7 s\n",
      "Wall time: 28.7 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## ПОЛ. \n",
    "# Векторайзер. Здесь воспользуемся TF-IDF (без нормализации, как завещал Петр)\n",
    "# Увеличение числа фич ничего не дает. Только замедляет. Поэтому остановимся на 25К\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "tv = TfidfVectorizer(analyzer='word',stop_words=stop_w,max_features = 25000,norm=None)\n",
    "# Сразу и посчитаем матрицу для известных данных\n",
    "t_matrix_train =  tv.fit_transform(pKnown['Host'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.8 ms, sys: 0 ns, total: 26.8 ms\n",
      "Wall time: 25.6 ms\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Конвертируем текст пола в число\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "le_gender = LabelEncoder()\n",
    "y_train_text =pKnown[\"Gender\"] \n",
    "Y=le_gender.fit_transform(y_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.4/dist-packages/sklearn/cross_validation.py:41: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/usr/local/lib/python3.4/dist-packages/sklearn/grid_search.py:42: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 8 candidates, totalling 40 fits\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.15, score=0.765465 - 6.0min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.15 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  6.0min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.15, score=0.775751 - 6.3min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.15 ...............\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed: 12.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.15, score=0.767470 - 4.1min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.15, score=0.776692 - 3.8min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.15, score=0.775001 - 3.8min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.15, score=0.766223 - 4.4min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.15, score=0.776265 - 4.4min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.15, score=0.769665 - 4.3min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.15, score=0.776409 - 4.5min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.15, score=0.776565 - 4.4min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.15, score=0.767733 - 4.7min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.15, score=0.777339 - 4.4min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.15, score=0.768181 - 4.5min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.15, score=0.776733 - 4.4min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.15, score=0.778642 - 4.4min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.15, score=0.767836 - 5.2min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.15, score=0.777287 - 5.3min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.15, score=0.768746 - 5.3min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.15, score=0.777461 - 5.1min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.15 ...............\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.15, score=0.778575 - 5.4min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.2, score=0.765819 - 3.7min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.2, score=0.775348 - 3.6min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.2, score=0.766912 - 3.9min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.2, score=0.778082 - 3.9min\n",
      "[CV] n_estimators=600, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=5, learning_rate=0.2, score=0.776970 - 3.6min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.2, score=0.764897 - 5.0min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.2, score=0.775757 - 4.2min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.2, score=0.766824 - 4.3min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.2, score=0.778086 - 5.5min\n",
      "[CV] n_estimators=700, max_depth=5, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=5, learning_rate=0.2, score=0.776837 - 5.0min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.2, score=0.766462 - 4.3min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.2, score=0.775039 - 5.1min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.2, score=0.766672 - 4.6min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.2, score=0.773983 - 4.3min\n",
      "[CV] n_estimators=600, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=600, max_depth=6, learning_rate=0.2, score=0.778067 - 4.3min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.2, score=0.766790 - 5.0min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.2, score=0.774777 - 5.1min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.2, score=0.766436 - 5.1min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.2, score=0.774651 - 4.5min\n",
      "[CV] n_estimators=700, max_depth=6, learning_rate=0.2 ................\n",
      "[CV]  n_estimators=700, max_depth=6, learning_rate=0.2, score=0.778786 - 3.1min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  40 out of  40 | elapsed: 182.7min finished\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 1d 12h 47min 52s, sys: 6min 34s, total: 1d 12h 54min 26s\n",
      "Wall time: 3h 5min 56s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "# Подбираем параметры\n",
    "################\n",
    "# Были протестированы в той или иной степени:\n",
    "# GradientBoostingClassifier\n",
    "# RandomForestClassifier\n",
    "# LinearSVC\n",
    "# DecisionTreeClassifier\n",
    "# AdaBoostClassifier\n",
    "# и самый унылый - LogisticRegression\n",
    "################\n",
    "# Тренируем. Гонять xgb в несколько потоков бессмысленно, он и так параллельный, поэтому n_jobs = 1.\n",
    "# Скорингом здесь поставим привычный roc_auc. Думал здесть поставить accuracy, но как-то прижился roc_auc\n",
    "mTrain=t_matrix_train\n",
    "mTrainTest=Y\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "import xgboost as xgb\n",
    "gstestG2 = GridSearchCV(\n",
    "    xgb.XGBClassifier(),\n",
    "    param_grid={'n_estimators':[600,700],\n",
    "                'learning_rate':[0.15,0.2],\n",
    "                'max_depth':[5,6]},\n",
    "    scoring='roc_auc',\n",
    "    cv=5,\n",
    "    verbose=3,\n",
    "    n_jobs=1)\n",
    "gstestG2.fit(mTrain, mTrainTest)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid - xgb.XGBClassifier\n",
      "0.7739810007076244\n",
      "{'n_estimators': 700, 'max_depth': 6, 'learning_rate': 0.15}\n"
     ]
    }
   ],
   "source": [
    "# Выведем результат. Скорее для себя. Фактически, грид это один их главных пожирателей времени\n",
    "print('grid - xgb.XGBClassifier')\n",
    "print(gstestG2.best_score_)\n",
    "print(gstestG2.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Сохранение работы\n",
    "pickle.dump(le_gender,open(sPath+'leGender','wb'))\n",
    "pickle.dump(tv,open(sPath+'tvMainF25k','wb'))\n",
    "pickle.dump(gstestG2,open(sPath+'gsXGB01','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "То же для возраста. У меня это делали разные ноутбуки. Даже на домашней анаконде тестировал. Главное - результат в пикл и собираем результат. Поэтому имена переменных будут повторяться"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 26.5 s, sys: 586 ms, total: 27.1 s\n",
      "Wall time: 27.1 s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "## Возраст.\n",
    "# Векторайзер. На возрасте прижился CV. Вообще, с возрастом сложно, там большой перекос в 24-35, плюс для грида немного\n",
    "# вариантов скореров. Я выбрал accuracy, в конце концов, нам надо нормально 50% предсказать, а не хорошо покрыть всех.\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "cv = CountVectorizer(max_features = 25000,stop_words=stop_w) \n",
    "t_matrix_train =  cv.fit_transform(pKnown['Host'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Конвертируем текст возраста в число\n",
    "le_age = LabelEncoder()\n",
    "y_train_text =pKnown[\"Age\"] \n",
    "Y=le_age.fit_transform(y_train_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 4 candidates, totalling 20 fits\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.436366 - 1.1min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   1 out of   1 | elapsed:  1.1min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.433043 - 1.2min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4 \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done   2 out of   2 | elapsed:  2.3min remaining:    0.0s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.457000 - 1.1min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.447769 - 1.2min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.444645 - 1.3min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.435120 - 1.7min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.434289 - 1.7min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.455339 - 2.1min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.446661 - 2.2min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.4, score=0.444229 - 2.1min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.437059 - 1.4min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.431935 - 1.3min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.453538 - 1.2min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.448462 - 1.2min\n",
      "[CV] estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=100, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.447970 - 1.2min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.435812 - 1.6min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.433181 - 1.6min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.457693 - 1.7min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.448324 - 1.6min\n",
      "[CV] estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3 \n",
      "[CV]  estimator__n_estimators=150, estimator__max_depth=2, estimator__learning_rate=0.3, score=0.445060 - 1.7min\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Parallel(n_jobs=1)]: Done  20 out of  20 | elapsed: 30.1min finished\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise',\n",
       "       estimator=OneVsRestClassifier(estimator=XGBClassifier(base_score=0.5, colsample_bylevel=1, colsample_bytree=1,\n",
       "       gamma=0, learning_rate=0.1, max_delta_step=0, max_depth=3,\n",
       "       min_child_weight=1, missing=None, n_estimators=100, nthread=-1,\n",
       "       objective='binary:logistic', reg_alpha=0, reg_lambda=1,\n",
       "       scale_pos_weight=1, seed=0, silent=True, subsample=1),\n",
       "          n_jobs=1),\n",
       "       fit_params={}, iid=True, n_jobs=1,\n",
       "       param_grid={'estimator__n_estimators': [100, 150], 'estimator__max_depth': [2], 'estimator__learning_rate': [0.4, 0.3]},\n",
       "       pre_dispatch='2*n_jobs', refit=True, scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Возрастов у нас пять, поэтому будет OneVsRest. Естественно с гридом. Обратите внимание, как передаются параметры\n",
    "# В OneVsRestClassifier, estimator - это сам XGBClassifier, поэтому параметры грда для него идут через __\n",
    "#\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "model_to_set = OneVsRestClassifier(estimator=xgb.XGBClassifier())\n",
    "parameters = {'estimator__max_depth': [2], 'estimator__learning_rate': [0.4,0.3], 'estimator__n_estimators': [100,150]}\n",
    "model_tunning = GridSearchCV(model_to_set, param_grid=parameters,\n",
    "                            scoring='accuracy',verbose=3,cv=5,n_jobs=1)\n",
    "model_tunning.fit(t_matrix_train, Y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "grid - OneVsRestClassifier -xgb.XGBClassifier\n",
      "0.4440135187545016\n",
      "{'estimator__n_estimators': 150, 'estimator__max_depth': 2, 'estimator__learning_rate': 0.3}\n"
     ]
    }
   ],
   "source": [
    "print('grid - OneVsRestClassifier -xgb.XGBClassifier')\n",
    "print(model_tunning.best_score_)\n",
    "print(model_tunning.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "## Сохранение работы\n",
    "pickle.dump(le_age,open(sPath+'le_age','wb'))\n",
    "pickle.dump(cv,open(sPath+'cvMainF25','wb'))\n",
    "pickle.dump(model_tunning,open(sPath+'ovrXGBA02.pcl','wb'))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Теперь сборка. Посчитаем результат и вероятности для каждого"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "##\n",
    "gstestG = pickle.load(open(sPath+'gsXGB01','rb'))\n",
    "tcv_gender = pickle.load(open(sPath+'tvMainF25k','rb'))\n",
    "le_gender = pickle.load(open(sPath+'leGender','rb'))\n",
    "classify_gender=gstestG.best_estimator_\n",
    "##\n",
    "gstestA = pickle.load(open(sPath+'ovrXGBA02.pcl','rb'))\n",
    "tcv_age = pickle.load(open(sPath+'cvMainF25','rb'))\n",
    "le_age = pickle.load(open(sPath+'le_age','rb'))\n",
    "classify_age=gstestA.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Предсказываем. Одного результата мало, нужны вероятности, т.к. нам достаточно хорошо предсказать 50%\n",
    "# Поэтому классификатор без predict_proba нам не нужны\n",
    "pUnKnown = pd.read_csv(sSecondUnKnown,encoding='utf-8')\n",
    "\n",
    "t_matrix_test =  tcv_gender.transform(pUnKnown['Host'])\n",
    "pred_gender = classify_gender.predict(t_matrix_test)\n",
    "pred_gender_pr = classify_gender.predict_proba(t_matrix_test)\n",
    "#\n",
    "t_matrix_age =  tcv_age.transform(pUnKnown['Host'])\n",
    "pred_age = classify_age.predict(t_matrix_age)\n",
    "pred_age_pr = classify_age.predict_proba(t_matrix_age)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Обозначим как неопознанные те из них, cумма вероятностей которых меньше cредней суммы вероятности"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.4895840659737587, 0.48957688361406326]\n"
     ]
    }
   ],
   "source": [
    "# Возраст умножаем на .75, пол на .25, т.к. к пола всего два варианта и успешный гарантированно выше 50,\n",
    "# а у возраста - пять и выше 50 далеко не все. Перекос поставил такой,\n",
    "# результат порадовал, поэтому подбирать дальше не стал\n",
    "aS=[0]*5000\n",
    "for i in range(5000):\n",
    "    aS[i] = max(pred_age_pr[i])*.75 + max(pred_gender_pr[i])*.25\n",
    "print(sorted(aS, reverse=True)[2499:2501])\n",
    "# Берем элемент [2499] и подставляем в формулу ниже"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "all_labels_age = le_age.inverse_transform(pred_age)\n",
    "all_labels_gender = le_gender.inverse_transform(pred_gender)\n",
    "for i in range(5000):\n",
    "    if max(pred_age_pr[i])*.75 + max(pred_gender_pr[i])*.25 < 0.4895840659737587:\n",
    "        all_labels_age[i]='-' \n",
    "        all_labels_gender[i]='-'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Сборка файла\n",
    "rr = pd.DataFrame(pUnKnown.Uid)\n",
    "rr['uid']=rr['Uid']\n",
    "rr['gender'] = all_labels_gender\n",
    "rr['age'] = all_labels_age\n",
    "rr.sort_values('uid',inplace=True)\n",
    "rr.to_csv('/data/home/dmitry.lyubinskiy/project01_gender-age.csv', header=True, sep='\\t', columns=['uid', 'gender', 'age'], index=False)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "------------------------\n",
    "time: 2017-06-11 10:37\n",
    "correctly predicted users / total number of users: 0.189\n",
    "part of users with predicted gender + age: 0.5\n",
    "correctly predicted users / number of predicted users: 0.378"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
